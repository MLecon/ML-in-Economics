{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# D200, Problem Set 1: Gradient Descent\n",
        "\n",
        "Due: 4 February 2025 [here](https://classroom.github.com/a/Ill2aQrC) in\n",
        "groups of 4.\n",
        "\n",
        "Stefan Bucher\n",
        "\n",
        "This problem set will introduce you to the gradient descent algorithm to\n",
        "solve the linear regression problem discussed in the lecture.[1] We will\n",
        "consider a univariate linear regression and seek to fit the parameters\n",
        "$\\theta_0$ and $\\theta_1$.\n",
        "\n",
        "# Problem 1\n",
        "\n",
        "**(1a)** Write down the mean-squared error function and its gradient\n",
        "with respect to the parameters.\n",
        "\n",
        "**(1b)** Implement the loss function `loss(X,y,theta)` and gradient\n",
        "function `gradient(X,y,theta)` in Python.\n",
        "\n",
        "# Problem 2\n",
        "\n",
        "**(2a)** Implement the simple gradient descent algorithm as a Python\n",
        "function\n",
        "`gradient_descent(X, y, theta_init, alpha, maxsteps, precision)`. The\n",
        "function should take the data $X$, $y$, initial parameters $\\theta$,\n",
        "step size, maximum number of steps, and a precision tolerance parameter.\n",
        "The function should return the history of the parameters, the cost at\n",
        "each step, and the predictions at each step.\n",
        "\n",
        "Then, use this function to print the resulting estimate, and plot the\n",
        "evolution of the parameters and the mean-squared loss over 2000\n",
        "iterations, with initial parameters $\\theta_0=\\theta_1=0$ and learning\n",
        "rate $\\alpha=0.01$.\n",
        "\n",
        "**(2b)** Compare the resulting estimates with those generated by\n",
        "`scipy.stats`, `statsmodels`, or `sklearn`. Plot the best-fitting line\n",
        "along with a scatterplot of the data.\n",
        "\n",
        "# Problem 3\n",
        "\n",
        "Visualize the loss surface, using the provided helper functions.\n",
        "\n",
        "[1] The problem set is adapted from Chi Jinâ€™s OxML 2024 Fundamentals\n",
        "class on Optimization."
      ],
      "id": "c5e7c7dc-070f-41a1-8b2b-40ff48799cce"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def plotting(history, cost):\n",
        "    theta1s = np.linspace(slope - 40 , slope + 30, 50)\n",
        "    theta0s = np.linspace(intercept - 40 , intercept + 30, 50)\n",
        "    M, B = np.meshgrid(theta1s, theta0s)\n",
        "    zs = np.array([loss(X, y, theta)\n",
        "                  for theta in zip(np.ravel(B), np.ravel(M))])\n",
        "    Z = zs.reshape(M.shape)\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot_surface(M, B, Z, cmap=cm.coolwarm, rstride=1, cstride=1, linewidth=0, color='white', alpha=0.8)\n",
        "    ax.contour(M, B, Z, 20, color='b', alpha=0.5, offset=0, stride=0)\n",
        "    ax.view_init(elev=15., azim=73)\n",
        "    ax.plot([history[-1][1]], [history[-1][0]], [cost[-1]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=10) # last point\n",
        "    ax.plot([history[0][1]], [history[0][0]], [cost[0]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=10) # first point\n",
        "\n",
        "    ax.plot([t[1] for t in history], [t[0] for t in history], cost , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=15) # cost history\n",
        "    ax.plot([t[1] for t in history], [t[0] for t in history], 0 , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=5) # projection on the origin plane\n",
        "\n",
        "    ax.set_xlabel(r'$\\theta_1$', fontsize=24)\n",
        "    ax.set_ylabel(r'$\\theta_0$', fontsize=24)\n",
        "    ax.set_title(f\"Iteration: {len(history)}\", fontsize=24, fontweight='bold')\n",
        "\n",
        "\n",
        "def plotting_diverge(history, cost):\n",
        "    #print(\"X: \", theta[1],\" abs(theta[1]) :\", abs(theta[1]))\n",
        "    theta1s = np.linspace(  -(-0.34 + abs(theta[1])*0.6 + 50) , -0.34 + abs(theta[1])*0.6 +50, 50)\n",
        "    theta0s = np.linspace(79.18 - abs(theta[1]) -200, 79.18 + abs(theta[1]) +200, 50)\n",
        "    M, B = np.meshgrid(theta1s, theta0s)\n",
        "    zs = np.array([loss(X, y, theta)\n",
        "                  for theta in zip(np.ravel(B), np.ravel(M))])\n",
        "    Z = zs.reshape(M.shape)\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot_surface(M, B, Z, cmap=cm.coolwarm, rstride=1, cstride=1, color='b', alpha=0.8)\n",
        "    ax.contour(M, B, Z, 20, color='b', alpha=0.5, offset=0, stride=0)\n",
        "    ax.view_init(elev=15., azim=73)\n",
        "    ax.plot([history[0][1]], [history[0][0]], [cost[0]], markerfacecolor='r', markeredgecolor='r', marker='o', markersize=10)\n",
        "    ax.plot([history[0][1]], [history[0][0]], [cost[0]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=10)\n",
        "\n",
        "    ax.plot([t[1] for t in history], [t[0] for t in history], cost , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=15)\n",
        "    ax.plot([t[1] for t in history], [t[0] for t in history], 0 , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=5)\n",
        "\n",
        "    ax.set_xlabel(r'$\\theta_1$', fontsize=24)\n",
        "    ax.set_ylabel(r'$\\theta_0$', fontsize=24)\n",
        "    ax.set_title(f\"Iteration: {len(history)}\", fontsize=24, fontweight='bold')\n",
        "\n",
        "\n",
        "def gen_gif(method_name,h,c,div=False):\n",
        "  %matplotlib notebook\n",
        "  from tqdm import tqdm\n",
        "  #import os\n",
        "  # generate gif, run after the specific method run\n",
        "  theta =  h[-1]\n",
        "  method_name = method_name\n",
        "\n",
        "  # use tqdm to create a progress bar\n",
        "  print(\"Produce the pngs\")\n",
        "  for i in range(1,len(h)+1):\n",
        "    history_, cost_ = h[0:i], c[0:i]\n",
        "    if not div:\n",
        "      plotting(history_,cost_)\n",
        "    else:\n",
        "      plotting_diverge(history_,cost_)\n",
        "    plt.savefig(f'figures/{method_name}-{i}.png')\n",
        "\n",
        "  import imageio\n",
        "  import os\n",
        "  print(\"Produce the GIF\")\n",
        "  with imageio.get_writer(f'figures/{method_name}.gif', mode='I') as writer:\n",
        "      for filename in tqdm([f'figures/{method_name}-{i}.png' for i in range(1,len(h)+1)], desc=\"Creating GIF\"):\n",
        "          image = imageio.imread(filename)\n",
        "          writer.append_data(image)\n",
        "          os.remove(filename)\n",
        "\n",
        "\n",
        "def plotting_loss_surface():\n",
        "    theta1s = np.linspace(slope - 40 , slope + 30, 50)\n",
        "    theta0s = np.linspace(intercept - 40 , intercept + 30, 50)\n",
        "    M, B = np.meshgrid(theta1s, theta0s)\n",
        "    zs = np.array([loss(X, y, theta)\n",
        "                  for theta in zip(np.ravel(B), np.ravel(M))])\n",
        "    Z = zs.reshape(M.shape)\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.plot_surface(M, B, Z, cmap=cm.coolwarm, rstride=1, cstride=1, linewidth=0, color='b', alpha=0.8)\n",
        "    ax.contour(M, B, Z, 20, color='b', alpha=0.5, offset=0, stride=0)\n",
        "    ax.view_init(elev=15., azim=73)\n",
        "\n",
        "    ax.set_xlabel(r'$\\theta_1$',fontsize=24)\n",
        "    ax.set_ylabel(r'$\\theta_0$',fontsize=24)"
      ],
      "id": "31b95e73"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use function `plotting(history, costs)` to visualize the loss surface\n",
        "for various values of the hyperparameter `alpha` to get an intuition for\n",
        "how the learning rate affects the performance of the algorithm. Discuss\n",
        "your observations."
      ],
      "id": "29ed4acb-e81a-4f2a-bce7-77fe002b258c"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/Users/sfb41/.pyenv/versions/MLenv/share/jupyter/kernels/python3"
    }
  }
}